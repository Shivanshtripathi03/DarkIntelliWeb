from flask import Flask, render_template, request, jsonify
import torch
from transformers import AutoTokenizer, AutoModel
import json

app = Flask(__name__)

# Load dataset and embeddings once at startup
with open("data/dataset.json", "r") as f:
    dataset = json.load(f)

embeddings = torch.load("data/embeddings/text_embeddings.pt")
tokenizer = AutoTokenizer.from_pretrained("models/bert-base-uncased")
model = AutoModel.from_pretrained("models/bert-base-uncased")

def query(text, top_k=1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    with torch.no_grad():
        query_emb = model(**inputs).last_hidden_state[:,0,:]  # CLS token
    sims = torch.nn.functional.cosine_similarity(query_emb, embeddings)
    top_indices = sims.topk(top_k).indices.tolist()
    results = [dataset[i]["text"] for i in top_indices]
    return results

@app.route('/')
def home():
    return "<h2>DarkIntelliWeb Dashboard Running Successfully ðŸš€</h2>"

@app.route('/ask', methods=['POST'])
def ask():
    data = request.json
    question = data.get("question", "")
    answers = query(question)
    return jsonify({"answers": answers})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
